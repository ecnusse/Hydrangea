app: AutoGPTArenaHack
repo: https://github.com/ATheorell/AutoGPTArenaHack
commit: 1e4f2dc004b92b9f236543674f94fb9f0af9bb2e
defect_id: ATheorell-AutoGPTArenaHack-exceeding_llm_content_limit-case10
type: exceeding LLM content limit
case: '10'
consequence:
- ST
locations:
- autogpts/autogpt/autogpt/processing/text.py (split_text, summarize_text); autogpts/autogpt/autogpt/config/config.py
  (__init__ token/model settings)
trigger_tests:
- '1. Set up AutoGPT (v<=0.2.1 per report) 2. Configure llm_utils/text per issue #2366
  (see original report) 3. Use a very large txt as input to summarization/processing
  4. Observe: InvalidRequestError maximum context length 8191 (requested ~15117).
  Quick simulation: run ''python test_text_summarization_token_overflow.py'' to reproduce
  overflow with chunked summaries'
