app: Auto-GPT-llama-cpp
repo: https://github.com/edik7333/Auto-GPT-llama-cpp
commit: 2b4bf189825c0abe28bf6f020b87921d82636d67
defect_id: edik7333-Auto-GPT-llama-cpp-knowledge_misalignment-case2
type: knowledge misalignment
case: '2'
consequence:
- IC
locations:
- scripts/browse.py/split_text()
trigger_tests:
- '1.Configure and start Auto-GPT. 2.Make a file of around 126 characters. 3.Run data_ingestion.py
  on a file, using the "local" memory backend. Also edit the file to fix Auto GPT
  data_ingestion is not working(Added the line: cfg.workspace_path = Path(file).parent
  / "autogpt/auto_gpt_workspace" to data_ingestion.py) and target auto-gpt.json in
  the workspace (this is where AutoGPT generates it if absent). 4.Check the auto-gpt.json
  file. It will not have any contents. 5.Check the console logs. They will indicate
  that 0 chunks were saved. 6.Copy some text into the file being ingested, to bring
  it to around 1198 characters, and re-run data_ingestion.py 7.This iteration has
  1 chunk and changes the auto-gpt.json file.'
